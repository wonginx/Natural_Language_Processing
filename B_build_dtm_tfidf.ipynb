{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "B_build_dtm_tfidf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wozzin/Natural_Language_Processing/blob/main/B_build_dtm_tfidf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iUsnxOyHhD4",
        "outputId": "37bd85f7-9d79-4920-f742-10e2df8c8e77"
      },
      "source": [
        "# install & import the libraries needed\n",
        "!pip3 install pandas\n",
        "!pip3 install scikit-learn\n",
        "from typing import List\n",
        "from math import log\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# for printing out all the columns of a pandas dataframe https://towardsdatascience.com/how-to-show-all-columns-rows-of-a-pandas-dataframe-c49d4507fcf\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg4U3QY2IB_v"
      },
      "source": [
        "# The corpus\n",
        "CORPUS = [\n",
        "    'this is the first document',\n",
        "    'the first document is this',\n",
        "    'this is the second document',\n",
        "    'and this is the third document',\n",
        "    'is this the first document'\n",
        "]\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ2uAyiBIGbD"
      },
      "source": [
        "# we will reuse the functions we completed in the previous tutorial\n",
        "def tf(term: str, doc: str) -> int:\n",
        "    # typing을 하는 이유: 오류를 미연에 방지하기 위해서 (단, IDE를 사용하는 경우에만)\n",
        "    # 더 정확한 버전 (is가 this에 포함 문제가 되므로)\n",
        "    tf = sum([\n",
        "      word == term     \n",
        "      for word in doc.split(\" \")  # word count\n",
        "    ])\n",
        "    return tf\n",
        "\n",
        "\n",
        "def build_dtm(corpus: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    we reuse what we implemented in previous tutorial\n",
        "    :param corpus:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # build vocabulary - use nested list comprehension, set, and list\n",
        "    words = [\n",
        "        word\n",
        "        for doc in corpus\n",
        "        for word in doc.split(\" \")\n",
        "    ]\n",
        "    vocab = list(set(words))\n",
        "\n",
        "    # populate dtm, ith a nested for loop\n",
        "    dtm = [\n",
        "           [tf(term, doc) for term in vocab]  # row\n",
        "           for doc in corpus\n",
        "    ]\n",
        "\n",
        "    # return dtm as a pandas dataframe\n",
        "    dtm = pd.DataFrame(data=dtm, columns=vocab)\n",
        "    return dtm\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5888fENLIQqX"
      },
      "source": [
        "# complete this function\n",
        "def idf(term: str, corpus: List[str]) -> float:\n",
        "    ### TODO 1 ###\n",
        "    # idf = log(n / (1 + df))\n",
        "    n = len(corpus)\n",
        "    # df: term이 출현한 문서의 빈도수\n",
        "    # 김영학09:04 split(\" \")을 사용 안 하면 문제가 생길까요?\n",
        "    # 넵, e.g. is ,this\n",
        "    df = sum([\n",
        "          term in doc.split(\" \")  # T, F\n",
        "          for doc in corpus\n",
        "    ])\n",
        "    idf = log(n / (1 + df))\n",
        "    ##############\n",
        "    return idf"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qUqm_Sh2Huu",
        "outputId": "d7f71f4c-4b63-4295-bd8a-d3a23efee649",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(idf(\"this\", CORPUS))\n",
        "print(idf(\"eubin\", CORPUS))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.1823215567939546\n",
            "1.6094379124341003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ4-x2kMJsnl"
      },
      "source": [
        "def build_dtm_with_tfidf(corpus: List[str]) -> pd.DataFrame:\n",
        "    # access the columns with df.columns\n",
        "    # multiply idfs to each row with numpy's broadcast mul\n",
        "    dtm = build_dtm(corpus) # tf\n",
        "    ### TODO 2 ####\n",
        "    # reuse dtm to build dtm_tfidf\n",
        "    # use dtm.columns, idf, np.array(), dtm.to_numpy() and broadcast multiplication\n",
        "    idfs: List[float] = [\n",
        "                         idf(term, corpus)\n",
        "                         for term in dtm.columns\n",
        "    ]\n",
        "    dtm_tfidf: np.array = dtm.to_numpy() * np.array(idfs)  # 리스트, 혹은 넘파이 배열\n",
        "    ############### \n",
        "    return pd.DataFrame(data=dtm_tfidf, columns=dtm.columns)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB_5uAIrKPqf",
        "outputId": "46481174-3c79-4d3b-ceee-2a1a0debc35d"
      },
      "source": [
        "# build dtm, and one with  \n",
        "dtm = build_dtm(CORPUS)\n",
        "dtm_tfidf = build_dtm_with_tfidf(CORPUS)\n",
        "print(dtm)\n",
        "print(dtm_tfidf)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   the  first  third  this  document  second  and  is\n",
            "0    1      1      0     1         1       0    0   1\n",
            "1    1      1      0     1         1       0    0   1\n",
            "2    1      0      0     1         1       1    0   1\n",
            "3    1      0      1     1         1       0    1   1\n",
            "4    1      1      0     1         1       0    0   1\n",
            "        the     first     third      this  document    second       and  \\\n",
            "0 -0.182322  0.223144  0.000000 -0.182322 -0.182322  0.000000  0.000000   \n",
            "1 -0.182322  0.223144  0.000000 -0.182322 -0.182322  0.000000  0.000000   \n",
            "2 -0.182322  0.000000  0.000000 -0.182322 -0.182322  0.916291  0.000000   \n",
            "3 -0.182322  0.000000  0.916291 -0.182322 -0.182322  0.000000  0.916291   \n",
            "4 -0.182322  0.223144  0.000000 -0.182322 -0.182322  0.000000  0.000000   \n",
            "\n",
            "         is  \n",
            "0 -0.182322  \n",
            "1 -0.182322  \n",
            "2 -0.182322  \n",
            "3 -0.182322  \n",
            "4 -0.182322  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oAQVQquMV2G"
      },
      "source": [
        "다음과 같은 결과가 나와야 합니다 (단어의 순서는 달라도 괜찮습니다):\n",
        "```\n",
        "   the  and  third  this  is  first  document  second\n",
        "0    1    0      0     1   2      1         1       0\n",
        "1    1    0      0     1   2      1         1       0\n",
        "2    1    0      0     1   2      0         1       1\n",
        "3    1    1      1     1   2      0         1       0\n",
        "4    1    0      0     1   2      1         1       0\n",
        "        the       and     third      this        is     first  document  \\\n",
        "0 -0.182322  0.000000  0.000000 -0.182322 -0.364643  0.223144 -0.182322   \n",
        "1 -0.182322  0.000000  0.000000 -0.182322 -0.364643  0.223144 -0.182322   \n",
        "2 -0.182322  0.000000  0.000000 -0.182322 -0.364643  0.000000 -0.182322   \n",
        "3 -0.182322  0.916291  0.916291 -0.182322 -0.364643  0.000000 -0.182322   \n",
        "4 -0.182322  0.000000  0.000000 -0.182322 -0.364643  0.223144 -0.182322   \n",
        "\n",
        "     second  \n",
        "0  0.000000  \n",
        "1  0.000000  \n",
        "2  0.916291  \n",
        "3  0.000000  \n",
        "4  0.000000  \n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSEEqll0Kp--"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SsHSXRkKTLK",
        "outputId": "5827234f-c8f6-44f1-b7b9-66f3318adceb"
      },
      "source": [
        "# compare the two\n",
        "print(cosine_similarity(dtm.to_numpy(), dtm.to_numpy()))\n",
        "print(cosine_similarity(dtm_tfidf.to_numpy(), dtm_tfidf.to_numpy()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         1.         0.8        0.73029674 1.        ]\n",
            " [1.         1.         0.8        0.73029674 1.        ]\n",
            " [0.8        0.8        1.         0.73029674 0.8       ]\n",
            " [0.73029674 0.73029674 0.73029674 1.         0.73029674]\n",
            " [1.         1.         0.8        0.73029674 1.        ]]\n",
            "[[1.         1.         0.31538537 0.23104796 1.        ]\n",
            " [1.         1.         0.31538537 0.23104796 1.        ]\n",
            " [0.31538537 0.31538537 1.         0.10015744 0.31538537]\n",
            " [0.23104796 0.23104796 0.10015744 1.         0.23104796]\n",
            " [1.         1.         0.31538537 0.23104796 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzdJrp3mMfPg"
      },
      "source": [
        "다음과 같은 결과가 나와야 합니다:\n",
        "```\n",
        "[[1.         1.         0.875      0.82495791 1.        ]\n",
        " [1.         1.         0.875      0.82495791 1.        ]\n",
        " [0.875      0.875      1.         0.82495791 0.875     ]\n",
        " [0.82495791 0.82495791 0.82495791 1.         0.82495791]\n",
        " [1.         1.         0.875      0.82495791 1.        ]]\n",
        "[[1.         1.         0.4227912  0.31662902 1.        ]\n",
        " [1.         1.         0.4227912  0.31662902 1.        ]\n",
        " [0.4227912  0.4227912  1.         0.16251445 0.4227912 ]\n",
        " [0.31662902 0.31662902 0.16251445 1.         0.31662902]\n",
        " [1.         1.         0.4227912  0.31662902 1.        ]]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg2616UaMrg6"
      },
      "source": [
        "## 다음의 질문에 답해주세요!\n",
        "\n",
        "> `dtm`으로 구한 유사도 행렬 대비, `dtm_tfidf`로 구한 유사도 행렬은 어떤 점이 개선되었나요? 그 이유는?\n",
        "\n",
        "- 의미 구분을 더 잘하게 되었다. 김영학09:11 - hit the nail on the head!\n",
        "의미가 같지 않은 문장끼리의 유사성이 1에서 멀어짐\n",
        "유사성이 1에서 멀어진 이유는 흔하게 나타나는(빈도수가 높은) 단어에 대해 가중치를 낮추어 주어서 반영하였기 때문\n",
        "\n",
        "\n",
        "> `dtm_tfidf`로도 해결할수 없는 문제를 발견할 수 있나요?\n",
        "- 카운트 기반 -> 순서를 고려 x (bag-of-words) / 순서를 고려하는 모델 (e.g. RNN, transformer, BERT)\n",
        "- 1. 순서를 고려하지 않아서 this is the first document의 평서문 문장과 비록 ?가 없지만 어순상 is this the first document의 의문문에 대한 유사성 1로 나타나 똑같은 의미를 갖는 문장으로 판단함\n",
        "-  distributional semantics: 의미 <- 데이터, 데이터에 문제가 있으면 의미에도 문제.\n",
        "실제로 중요하지 않은 단어 'and'의 df가 1로 흔하지 않은(빈도수가 낮은) 단어로 판단함. 즉, 가중치가 높은 중요한 단어로 고려됨. 주어진 데이터에 따라 가중치가 결정됨.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEGqECWqKo50"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}